class CharModel(nn.Module):
  def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False):

    super().__init__()

    self.drop_prob = drop_prob
    self.num_layers = num_layers 
    self.num_hidden = num_hidden
    self.use_gpu = use_gpu 

    self.all_chars = all_chars 
    self.decoder = dict(enumerate(all_chars))
    self.encoder = {char:ind for ind,char in decoder.items()}

    self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)

    self.dropout = nn.Dropout(drop_prob)

    self.fc_linear = nn.Linear(num_hidden, len(self.all_chars)) 

def forward(self, x, hidden):
  lstm_output, hidden = self.lstm(x, hidden)

  drop_output = self.dropout(lstm_output)

  drop_output = dropout.contiguous().view(-1, self.num_hidden)

  final_out = self.fc_linear(drop_output)

  return final_out,hidden 

def hidden_state(self, batch_size):
  if self.use_gpu:
    hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),
              torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())
  else: 
    hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),
              torch.zeros(self.num_layers, batch_size, self.num_hidden))

  return hidden 

model = CharModel(all_chars=all_characters, num_hidden=512, num_layers=3, drop_prob=0.5,use_gpu=True)

total_param = []

for p in model.parameters():
  total_param.append(int(p.numel()) 

sum(total_param) #same of length of text or data 

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss() 

train_percent = 0.1 
train_ind = 0.1 
train_data = encoded_text[:train_ind]
val_data = encoded_text[train_ind:]

train_percent = 0.9
train_ind = int(len(encoded_text)*train_percent)*train_percent)
train_data = encoded_text[:train_ind]
val_data = encoded_text[train_ind:]

epochs = 60
batch_size = 100 
seq_len = 100

tracker = 0

num_char = max(encoded_text)+1

model.train()

if model.use_gpu:
  model.cuda()

for i in range(epochs):
  hidden = model.hidden_state(batch_size)
  for x,y in generate_batches(train_data,batch_size,seq_len):
    tracker += 1
    x = one_hot_encoder(x,num_char)
    inputs = torch.from_numpy(x)
    targets = torch.from_numpy(y)

    if model.use_gpu:
      inputs = inputs.cuda()
      targets = targets.cuda()

     hidden = tuple([state.data for state in hidden]) 

     model.zero_grad()

     lstm_output,hidden = model.forward(inputs, hidden)
     loss = critersion(lstm_output, targets.view(batch_size*seq_len).long())
     loss.backward()
     nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)

     optimizer.step()

     if tracker % 25 == 0:
       val_hidden = model.hidden_state(batch_size)
       val_losses = []
       model.eval()

       for x,y in generate_batches(val_data,batch_size,seq_len):
         x = one_hot_encoder(x,num_char)
         inputs = torch.from_numpy(x)
         targets = torch.from_numpy(y)

        if model.use_gpu:
          inputs = inputs.cuda()
          targets = targets.cuda()

        val_hidden = tuple([state.data for state in val_hidden])
        lstm_output, val_hidden = model.forward(inputs,val_hidden)
        val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())

        val_losses.append(val_loss.item())

       model.train()
       print(f"EPOCH: {i} Step: {tracker} VAL LOSS: {val_loss.item()}")

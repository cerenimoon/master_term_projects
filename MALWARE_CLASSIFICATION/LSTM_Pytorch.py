sequence_len = 28
input_len = 28
hidden_size = 128
num_layers = 2
num_classes = 10 
num_epochs = 5 
learning_rate = 0.01 #hyperparameters 

class LSTM(nn.Module):
  def __init__(self, input_len,hidden_size, num_class, num_layers):
    super(LSTM, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers 
    self.lstm = nn.LSTM(input_len, hidden_size, num_layers, batch_first=True)
    self.output_layer = nn.Linear(hidden_size, num_classes)

  def forward(self, X):
    hidden_states = torch.zeros(self.num_layers, X.size(0),self.hidden_size)
    cell_states = torch.zeros(self.num_layers, X.size(0),self.hidden_size)
    out, _ = self.lstm(X, (hidden_states, cell_states))
    out = self.output_layer(out[:, -1, :])
    return out

model = LSTM(input_len, hidden_size, num_classes, num_layers)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

def train(num_epochs, model, train_dataloader, loss_func):
  total_steps = len(train_dataloader)
  
  for epoch in range(num_epochs):
    for batch, (images, labels) in enumerate(train_dataloader):
      images = images.reshape(-1, sequence_len, input_len)
      #labels = labels.cuda()
    
      output = model(images)
      loss = loss_func(output, labels)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      if (batch+1)%100 == 0:
        print(f"Epoch: {epoch+1}; Batch {batch+1} / {total_steps}; Loss: {loss.item():>4f}")


train(num_epochs, model, train_dataloader, loss_func)

test_images, test_labels = next(iter(test_dataloader))
test_labels

test_output = model(test_images.view(-1,28,28))

predicted = torch.max(test_output, 1)

    

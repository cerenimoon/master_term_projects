# Define the custom EfficientNet model
class CustomEfficientNet(nn.Module):
    def __init__(self, num_classes=9):
        super(CustomEfficientNet, self).__init__()
        self.efficientnet = models.efficientnet_b5(weights='IMAGENET1K_V1')
        
        # Replace the Global Average Pooling layer with Max Pooling
        self.efficientnet.avgpool = nn.MaxPool2d(kernel_size=7, stride=1) #7 1
        
        # Calculate the number of features after the Max Pooling layer
        # The kernel size used here is important to make sure the output size matches
        # Expected input size after MaxPool2d should match the linear layer input size
        num_ftrs = self.efficientnet.classifier[1].in_features
        
        # Adjust the classifier
        self.efficientnet.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(p=0.2, inplace=True),
            nn.Linear(num_ftrs, num_classes)
        )
    
    def forward(self, x):
        #x = self.efficientnet.features(x)
        #x = self.efficientnet.avgpool(x)
        #x = self.efficientnet.classifier(x)
        x = self.efficientnet(x)
        return x
        #return x

# Instantiate the model
model_ft = CustomEfficientNet(num_classes=9)
model_ft = model_ft.to(device)

# Define loss function
criterion = nn.CrossEntropyLoss()

# Define optimizer
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)

# Define learning rate scheduler
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

# Print model summary
print(model_ft)

#old code
model_ft = models.efficientnet_b5(weights='IMAGENET1K_V1')

# Freeze pre-trained layers (optional)
#for param in model_ft.parameters():
    #param.requires_grad = False  # Freeze pre-trained layers

# Get number of features from the pre-trained classifier
# Replace the Global Average Pooling layer with Max Pooling
model_ft.avgpool = nn.MaxPool2d(kernel_size=7, stride=1) #7 1
        
# Calculate the number of features after the Max Pooling layer
# The kernel size used here is important to make sure the output size matches
# Expected input size after MaxPool2d should match the linear layer input size
num_ftrs = model_ft.classifier[1].in_features


# Define your custom dense layers (adjust as needed)
model_ft.classifier = nn.Sequential(
    nn.Flatten(),
    nn.Dropout(p=0.2, inplace=True),
    nn.Linear(num_ftrs, 9),     
)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that only parameters in the new classifier module are optimized
optimizer_ft = optim.SGD(model_ft.classifier.parameters(), lr=0.01, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)
